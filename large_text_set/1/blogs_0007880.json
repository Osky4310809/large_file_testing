{"organizations": [], "uuid": "401d6db12c659dbe2dfb1cf7d9d5b47bb58ee1bf", "thread": {"social": {"gplus": {"shares": 9}, "pinterest": {"shares": 1}, "vk": {"shares": 0}, "linkedin": {"shares": 7}, "facebook": {"likes": 261, "shares": 261, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "www.economist.com", "main_image": "http://cdn.static-economist.com/sites/default/files/cf_images/images-magazine/2017/01/14/BL/20170114_BLP508_facebook.jpg", "site_section": "http://www.economist.com/blogs/economist-explains/index.xml", "section_title": "", "url": "http://www.economist.com/blogs/economist-explains/2017/01/economist-explains-6", "country": "US", "domain_rank": 1742, "title": "How machines learned to speak human language", "performance_score": 2, "site": "economist.com", "participants_count": 1, "title_full": "The Economist explains: How machines learned to speak human language | The Economist", "spam_score": 0.0, "site_type": "blogs", "published": "2017-01-11T19:36:00.000+02:00", "replies_count": 0, "uuid": "401d6db12c659dbe2dfb1cf7d9d5b47bb58ee1bf"}, "author": "R.L.G.", "url": "http://www.economist.com/blogs/economist-explains/2017/01/economist-explains-6", "ord_in_thread": 0, "title": "How machines learned to speak human language", "locations": [], "entities": {"persons": [{"name": "alexa", "sentiment": "none"}, {"name": "siri", "sentiment": "none"}], "locations": [{"name": "america", "sentiment": "none"}, {"name": "london", "sentiment": "none"}], "organizations": [{"name": "apple", "sentiment": "none"}, {"name": "microsoft", "sentiment": "none"}, {"name": "google", "sentiment": "none"}, {"name": "amazon", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "THIS past Christmas, millions of people will have opened boxes containing gadgets with a rapidly improving ability to use human language. Amazon’s Echo device, featuring a digital assistant called Alexa, is now present in over 5m homes. The Echo is a cylindrical desktop computer with no interface apart from voice. Ask Alexa for the weather, to play music, to order a taxi, to tell you about your commute or to tell a corny joke, and she will comply. The voice-driven digital assistants from America’s computer giants (Google Assistant, Microsoft’s Cortana and Apple’s Siri) have also vastly improved. How did computers tackle the problems of human language?\nOnce, the idea was to teach machines rules—for example, in translation, a set of grammar rules for breaking down the meaning of the source language, and another set for reproducing the meaning in the target language. But after a burst of optimism in the 1950s, such systems could not be made to work on complex new sentences; the rules-based approach would not scale up. Funding for human-language technologies went into hibernation for decades, until a renaissance in the 1980s.\nIn effect, language technologies teach themselves, via a form of pattern-matching. For speech recognition, computers are fed sound files on the one hand, and human-written transcriptions on the other. The system learns to predict which sounds should result in what transcriptions. In translation, the training data are source-language texts and human-made translations. The system learns to match the patterns between them. One thing that improves both speech recognition and translation is a “language model”—a bank of knowledge about what (for example) English sentences tend to look like. This narrows the systems’ guesswork considerably. Three things have made this approach take a big leap forward recently: First, computers are far more powerful. Second, they can learn from huge and growing stores of data, whether publicly available on the internet or privately gathered by firms. Third, so-called “deep learning”, which uses digital neural networks with several layers of digital “neurons” and connections between them, have become very good at learning from example.\nAll this means that computers are now impressively competent at handling spoken requests that require a narrowly defined reply. “What’s the temperature going to be in London tomorrow?” is simple (To be fair, you don't need to be a computer to know it is going to rain in London tomorrow). Users can even ask in more natural ways, such as, “Should I carry an umbrella to London tomorrow?” (Digital assistants learn continually from the different ways people ask questions.) But ask a wide-open question (“Is there anything fun and inexpensive to do in London tomorrow?”) and you will usually just get a list of search-engine results. As machine learning improves, and as users let their gadgets learn more about them specifically, such answers will become more useful. This has implications that trouble privacy advocates, but if the past few years of mobile-phone use are any indication, consumers will be sufficiently delighted by the new features to make the trade-off.\n<div", "external_links": ["http://cdn.static-economist.com/sites/default/files/imagecache/full-width/20170114_BLP508.jpg"], "published": "2017-01-11T19:36:00.000+02:00", "crawled": "2017-01-11T08:36:24.931+02:00", "highlightTitle": ""}