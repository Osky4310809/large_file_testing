{"organizations": [], "uuid": "23f24e11f52209c7de1dc1e8d2f7e5dd00c27163", "thread": {"social": {"gplus": {"shares": 8}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 3}, "facebook": {"likes": 224, "shares": 224, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "arstechnica.com", "main_image": "https://cdn.arstechnica.net/wp-content/uploads/2017/03/14424470570_94590de996_k-760x380.jpg", "site_section": "http://feeds.arstechnica.com/arstechnica/index/", "section_title": "Ars Technica", "url": "https://arstechnica.com/science/2017/03/people-have-no-idea-which-sciences-are-robust/", "country": "US", "domain_rank": 1109, "title": "People have no idea which sciences are robust", "performance_score": 2, "site": "arstechnica.com", "participants_count": 8, "title_full": "People have no idea which sciences are robust | Ars Technica", "spam_score": 0.0, "site_type": "blogs", "published": "2017-03-04T05:55:00.000+02:00", "replies_count": 8, "uuid": "23f24e11f52209c7de1dc1e8d2f7e5dd00c27163"}, "author": "Cathleen O'Grady", "url": "https://arstechnica.com/science/2017/03/people-have-no-idea-which-sciences-are-robust/", "ord_in_thread": 0, "title": "People have no idea which sciences are robust", "locations": [], "entities": {"persons": [], "locations": [], "organizations": []}, "highlightText": "", "language": "english", "persons": [], "text": "If there’s one thing everyone needs to understand about science, it’s that science is uncertain. It’s a process of gradually getting closer to the truth, with self-correcting mechanisms built in. Unfortunately, communicating this uncertainty without undermining trust in science is tricky. People who hear that climate science has uncertainty often think “scientists aren’t so sure that climate change is happening,” when the reality is more like “climate scientists aren’t sure whether we’re looking at 2°C or 4°C of warming by 2100.”\nIf scientists understood better how people perceive uncertainty in science, they could do a better job of communicating their results and, perhaps, improving trust in science. With this in mind, Stephen Broomell and Patrick Bodilly Kane, two researchers at Carnegie Mellon University, have conducted a series of studies exploring how people understand scientific uncertainty. Their findings suggest that precision in particular seems to matter to people and that Republicans in particular attach a field’s value (including how much funding it should get) to its perceived precision.\nAnd it should come as no surprise that the same people have funny ideas about which sciences are precise.\nUncertainty in science can creep in at different places. Perhaps the most obvious one is measurement error: how well-tuned are the instruments you’re using? But there are more complex areas of uncertainty, too: science generalizes from an observable sample of the data to entire populations; it draws on existing data to make projections; and there is often disagreement over how results should be interpreted.\nBroomell and Kane surveyed 217 people online to find out how they perceived uncertainty in a range of scientific fields. People were given a brief description of each field and then rated each field for 14 different dimensions of uncertainty. The questions covered factors like expert disagreement, measurement accuracy, over-generalization of results, and amount of abstraction. People were also asked to rate how valuable they thought each field is using a seven-point rating of the field’s quality, social benefit, influence over their personal decisions, and how much funding it should be given.\nThe results suggested that the dimensions of uncertainty that had something to do with precision (like randomness and measurement accuracy) played the biggest role in people’s perceptions. The fields that were labelled as “least precise” were psychology and evolution. Also on this end of the scale were economics, climate science, and—wait for it—astrophysics. On the other end, forensics was perceived as the field with the highest level of precision, followed by aerospace engineering.\nAs the authors point out, people's perceptions aren’t entirely in touch with reality. The result for forensic science, in particular, echoes “what has been coined the CSI effect, where legal scholars worry that juries have a misperception of the scientific precision of forensic analysis,” they write. They also note that classing medical research, seismology, and nuclear engineering at roughly similar levels of precision is giving a bit too much credit to seismology and medical research.\nDividing the participants up into Republicans and Democrats caused some small changes in how each discipline was labelled. Most notably, the Republicans placed evolution, psychology, and climatology at the lowest levels of precision. It’s not clear whether these differences are significant, though.\nThe important difference was that the Republicans, more than Democrats, considered the precise fields more valuable. The implication here is that perceptions of low precision in fields like evolution and climate science are directly related to Republicans’ ideas about how valuable those fields are to society.\nThe researchers then tried a couple of experiments in which they changed the name of a field when presenting some of its results. They gave a list of results from forensic psychology and labelled them as either forensics (which people think is very precise) or psychology (which has low precision). But they didn’t find much of a difference in how people perceived the results.\nBroomell and Kane suggest that this means that people have perceptions of an overall field but don’t apply this perception to individual results from that field. There’s an alternative explanation, though: the scientific results they mentioned were all to do with human behavior, so even if they labelled them as “forensics,” people who think of forensics as DNA analysis and fingerprinting might not have been fooled.\nBecause different countries tend to have different politicized areas of science, running the same experiment in other countries, as well as getting a larger sample in the US, would be good. In a US context, though, it has important significance for people’s perception of the value of various disciplines—and potentially for policy and funding decisions.\nJournal of Experimental Psychology , 2016. DOI: 10.1037/xge0000260 ( About DOIs ).", "external_links": [], "published": "2017-03-04T05:55:00.000+02:00", "crawled": "2017-03-04T01:35:27.597+02:00", "highlightTitle": ""}