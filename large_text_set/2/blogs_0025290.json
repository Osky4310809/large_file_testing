{"organizations": [], "uuid": "a8cad1546d1d240b593ef1bec1d3e8d4a5b7ed5d", "thread": {"social": {"gplus": {"shares": 2}, "pinterest": {"shares": 2}, "vk": {"shares": 0}, "linkedin": {"shares": 7}, "facebook": {"likes": 230, "shares": 230, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "www.collective-evolution.com", "main_image": "", "site_section": "http://www.collective-evolution.com", "section_title": "Collective Evolution", "url": "http://www.collective-evolution.com/2017/02/26/googles-artificial-intelligence-learns-highly-aggressive-behaviour-betrayal-pay-off/", "country": "US", "domain_rank": 10352, "title": "Google’s Artificial Intelligence Learns “Highly Aggressive” Behaviour & Betrayal Pay Off", "performance_score": 2, "site": "collective-evolution.com", "participants_count": 1, "title_full": "Google’s Artificial Intelligence Learns “Highly Aggressive” Behaviour & Betrayal Pay Off", "spam_score": 0.0, "site_type": "blogs", "published": "2017-02-26T23:39:00.000+02:00", "replies_count": 0, "uuid": "a8cad1546d1d240b593ef1bec1d3e8d4a5b7ed5d"}, "author": "Alexa Erickson", "url": "http://www.collective-evolution.com/2017/02/26/googles-artificial-intelligence-learns-highly-aggressive-behaviour-betrayal-pay-off/", "ord_in_thread": 0, "title": "Google’s Artificial Intelligence Learns “Highly Aggressive” Behaviour & Betrayal Pay Off", "locations": [], "entities": {"persons": [{"name": "joe leibo", "sentiment": "none"}], "locations": [], "organizations": [{"name": "google", "sentiment": "negative"}, {"name": "wolfpack", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "An artificial intelligence created by Google made headlines recently because of its ability to learn “highly aggressive” behaviour.\nDeepMind, Google’s cutting-edge AI company, has accomplished much thus far, including learning from its memory, mimicking human voices , writing music , and beating the best Go player in the world. Recently, the team behind the AI company ran a series of tests to see how it would respond to certain social dilemmas, specifically if it would be more likely to cooperate or compete.\nAmong the tests, one involved 40 million instances of playing the computer game Gathering , where DeepMind revealed just how far it will go to get what it wants. The game involved two scenarios — a wolf pack hunt requiring cooperation, and fruit gathering.\nThe fruit gathering scenario proved particularly intriguing, since the AI was given the ability to damage other intelligence by attacking them, or shooting beams at them. The team discovered that the AI became more aggressive through attacking mechanisms when less fruit was present.\nThe DeepMind team described the fruit game in a blog post :\nWe let the agents play this game many thousands of times and let them learn how to behave rationally using deep multi-agent reinforcement learning. Rather naturally, when there are enough apples in the environment, the agents learn to peacefully coexist and collect as many apples as they can. However, as the number of apples is reduced, the agents learn that it may be better for them to tag the other agent to give themselves time on their own to collect the scarce apples.\nThe team also acknowledged that the AI systems began to develop some forms of human behaviour , saying their model “ shows that some aspects of human-like behaviour emerge as a product of the environment and learning.” “Less aggressive policies,” on the other hand, “emerge from learning in relatively abundant environments with less possibility for costly action.” “The greed motivation reflects the temptation to take out a rival and collect all the apples oneself,” they continue. \nThe team explained that, with the Wolfpack game, “The idea is that the prey is dangerous, a lone wolf can overcome it, but is at risk of losing the carcass to scavengers. However, when the two wolves capture the prey together, they can better protect the carcass from scavengers and hence receive a higher reward.”\nThe researchers are now trying to figure out how AI can eventually “ control complex multi-agent systems such as the economy, traffic systems, or the ecological health of our planet – all of which depend on our continued cooperation.” \nPertaining to everyday life, such information could prove important to the design of self-driving cars, which will need to find the safest routes while also taking into consideration the intentions of all parties involved.\nThe tests suggest that if the objectives are not correctly balanced out in the programming, the AI might respond selfishly, which could be dangerous.\nAs for the next step for DeepMind, team member Joe Leibo envisions the AI going deeper into the motivations behind decision-making. He said , “Going forward it would be interesting to equip agents with the ability to reason about other agent’s beliefs and goals.” \n", "external_links": [], "published": "2017-02-26T23:39:00.000+02:00", "crawled": "2017-02-26T18:49:54.069+02:00", "highlightTitle": ""}