{"organizations": [], "uuid": "f5c1f65d402fc32638c23a8e984a497dbe5437d1", "thread": {"social": {"gplus": {"shares": 436}, "pinterest": {"shares": 81}, "vk": {"shares": 1}, "linkedin": {"shares": 379}, "facebook": {"likes": 115, "shares": 115, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "moz.com", "main_image": "https://d1avok0lzls2w.cloudfront.net/uploads/og_image/58d1eedddb3491.09385308.png", "site_section": "https://moz.com/blog/category/advanced-seo?page=11", "section_title": "Advanced SEO - page 11 - Moz", "url": "https://moz.com/blog/content-audit-tutorial", "country": "US", "domain_rank": 1643, "title": "How to Do a Content Audit [Updated for 2017]", "performance_score": 1, "site": "moz.com", "participants_count": 10, "title_full": "How to Do a Content Audit [Updated for 2017] - Moz", "spam_score": 0.996, "site_type": "blogs", "published": "2017-03-22T10:04:00.000+02:00", "replies_count": 10, "uuid": "f5c1f65d402fc32638c23a8e984a497dbe5437d1"}, "author": "Everett Sizemore", "url": "https://moz.com/blog/content-audit-tutorial", "ord_in_thread": 0, "title": "How to Do a Content Audit [Updated for 2017]", "locations": [], "entities": {"persons": [{"name": "botify", "sentiment": "none"}, {"name": "siteliner", "sentiment": "none"}, {"name": "brombone", "sentiment": "none"}, {"name": "bill slawski", "sentiment": "none"}], "locations": [{"name": "ga", "sentiment": "none"}], "organizations": [{"name": "ajax", "sentiment": "none"}, {"name": "gsc", "sentiment": "none"}, {"name": "swf", "sentiment": "none"}, {"name": "css", "sentiment": "none"}, {"name": "amazon", "sentiment": "none"}, {"name": "google", "sentiment": "none"}, {"name": "google analytics", "sentiment": "none"}, {"name": "ebay", "sentiment": "none"}, {"name": "new york times", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "This guide provides instructions on how to do a content audit using examples and screenshots from Screaming Frog, URL Profiler, Google Analytics (GA), and Excel, as those seem to be the most widely used and versatile tools for performing content audits.\n{Expand for more background}\nIt's been almost three years since the original “ How to do a Content Audit – Step-by-Step ” tutorial was published here on Moz, and it’s due for a refresh. This version includes updates covering JavaScript rendering, crawling dynamic mobile sites, and more.\nIt also provides less detail than the first in terms of prescribing every step in the process. This is because our internal processes change often, as do the tools. I’ve also seen many other processes out there that I would consider good approaches. Rather than forcing a specific process and publishing something that may be obsolete in six months, this tutorial aims to allow for a variety of processes and tools by focusing more on the basic concepts and less on the specifics of each step.\nWe have a DeepCrawl account at Inflow , and a specific process for that tool, as well as several others. Tapping directly into various APIs may be preferable to using a middleware product like URL Profiler if one has development resources. There are also custom in-house tools out there, some of which incorporate historic log file data and can efficiently crawl websites like the New York Times and eBay. Whether you use GA or Adobe Sitecatalyst, Excel, or a SQL database, the underlying process of conducting a content audit shouldn’t change much.\nTABLE OF CONTENTS\nWhat is an SEO content audit? What is the purpose of a content audit? How & why “pruning” works How to do a content audit The inventory & audit phase Step 1: Crawl all indexable URLs Crawling roadblocks & new technologies Crawling very large websites Crawling dynamic mobile sites Crawling and rendering JavaScript Step 2: Gather additional metrics Things you don’t need when analyzing the data The analysis & recommendations phase Step 3: Put it all into a dashboard Step 4: Work the content audit dashboard The reporting phase Step 5: Writing up the report Content audit resources & further reading What is a content audit? A content audit for the purpose of SEO includes a full inventory of all indexable content on a domain, which is then analyzed using performance metrics from a variety of sources to determine which content to keep as-is, which to improve, and which to remove or consolidate.\nWhat is the purpose of a content audit? A content audit can have many purposes and desired outcomes. In terms of SEO, they are often used to determine the following:\nHow to escape a content-related search engine ranking filter or penalty Content that requires copywriting/editing for improved quality Content that needs to be updated and made more current Content that should be consolidated due to overlapping topics Content that should be removed from the site The best way to prioritize the editing or removal of content Content gap opportunities Which content is ranking for which keywords Which content should be ranking for which keywords The strongest pages on a domain and how to leverage them Undiscovered content marketing opportunities Due diligence when buying/selling websites or onboarding new clients While each of these desired outcomes and insights are valuable results of a content audit, I would define the overall “purpose” of one as:\nThe purpose of a content audit for SEO is to improve the perceived trust and quality of a domain, while optimizing crawl budget and the flow of PageRank (PR) and other ranking signals throughout the site. Often, but not always, a big part of achieving these goals involves the removal of low-quality content from search engine indexes. I’ve been told people hate this word, but I prefer the “pruning” analogy to describe the concept.\nHow & why “pruning” works {Expand for more on pruning} Content audits allow SEOs to make informed decisions on which content to keep indexed “as-is,” which content to improve, and which to remove. Optimizing crawl budget and the flow of PR is self-explanatory to most SEOs. But how does a content audit improve the perceived trust and quality of a domain? By removing low-quality content from the index (pruning) and improving some of the content remaining in the index, the likelihood that someone arrives on your site through organic search and has a poor user experience (indicated to Google in a variety of ways ) is lowered. Thus, the quality of the domain improves. I’ve explained the concept here and here .\nOthers have since shared some likely theories of their own, including a larger focus on the redistribution of PR.\nCase study after case study has shown the concept of “pruning” (removing low-quality content from search engine indexes) to be effective, especially on very large websites with hundreds of thousands (or even millions) of indexable URLs. So why do content audits work? Lots of reasons. But really...\nDoes it matter?\n¯\\_( ツ )_/¯\nHow to do a content audit Just like anything in SEO, from technical and on-page changes to site migrations, things can go horribly wrong when content audits aren’t conducted properly. The most common example would be removing URLs that have external links because link metrics weren’t analyzed as part of the audit. Another common mistake is confusing removal from search engine indexes with removal from the website.\nContent audits start with taking an inventory of all content available for indexation by search engines. This content is then analyzed against a variety of metrics and given one of three “Action” determinations. The “Details” of each Action are then expanded upon.\nThe variety of combinations of options between the “Action” of WHAT to do and the “Details” of HOW (and sometimes why) to do it are as varied as the strategies, sites, and tactics themselves. Below are a few hypothetical examples:\nYou now have a basic overview of how to perform a content audit. More specific instructions can be found below.\nThe process can be roughly split into three distinct phases:\nInventory & audit Analysis & recommendations Summary & reporting The inventory & audit phase Taking an inventory of all content, and related metrics, begins with crawling the site.\nOne difference between crawling for content audits and technical audits:\nTechnical SEO audit crawls are concerned with all crawlable content (among other things).\nContent audit crawls for the purpose of SEO are concerned with all indexable content.\n{Expand for more on crawlable vs. indexable content}\nThe URL in the image below should be considered non-indexable. Even if it isn’t blocked in the robots.txt file, with a robots meta tag, or an X-robots header response –– even if it is frequently crawled by Google and shows up as a URL in Google Analytics and Search Console –– the rel =”canonical” tag shown below essentially acts like a 301 redirect, telling Google not to display the non-canonical URL in search results and to apply all ranking calculations to the canonical version. In other words, not to “index” it.\nI'm not sure “index” is the best word, though. To “display” or “return” in the SERPs is a better way of describing it, as Google surely records canonicalized URL variants somewhere, and advanced site: queries seem to show them in a way that is consistent with the \"supplemental index\" of yesteryear. But that's another post, more suitably written by a brighter mind like Bill Slawski.\nA URL with a query string that canonicalizes to a version without the query string can be considered “not indexable.”\nA content audit can safely ignore these types of situations, which could mean drastically reducing the amount of time and memory taken up by a crawl.\nTechnical SEO audits, on the other hand, should be concerned with every URL a crawler can find. Non-indexable URLs can reveal a lot of technical issues, from spider traps (e.g. never-ending empty pagination, infinite loops via redirect or canonical tag) to crawl budget optimization (e.g. How many facets/filters deep to allow crawling? 5? 6? 7?) and more.\nIt is for this reason that trying to combine a technical SEO audit with a content audit often turns into a giant mess , though an efficient idea in theory. When dealing with a lot of data, I find it easier to focus on one or the other: all crawlable URLs, or all indexable URLs.\nOrphaned pages (i.e., with no internal links / navigation path) sometimes don’t turn up in technical SEO audits if the crawler had no way to find them. Content audits should discover any indexable content, whether it is linked to internally or not. Side note: A good tech audit would do this, too.\nIdentifying URLs that should be indexed but are not is something that typically happens during technical SEO audits.\nHowever, if you're having trouble getting deep pages indexed when they should be, content audits may help determine how to optimize crawl budget and herd bots more efficiently into those important, deep pages. Also, many times Google chooses not to display/index a URL in the SERPs due to poor content quality (i.e., thin or duplicate).\nAll of this is changing rapidly, though . URLs as the unique identifier in Google’s index are probably going away . Yes, we’ll still have URLs, but not everything requires them. So far, the word “content” and URL has been mostly interchangeable. But some URLs contain an entire application’s worth of content. How to do a content audit in that world is something we’ll have to figure out soon, but only after Google figures out how to organize the web’s information in that same world. From the looks of things, we still have a year or two.\nUntil then, the process below should handle most situations.\nStep 1: Crawl all indexable URLs A good place to start on most websites is a full Screaming Frog crawl. However, some indexable content might be missed this way. It is not recommended that you rely on a crawler as the source for all indexable URLs.\nIn addition to the crawler, collect URLs from Google Analytics, Google Webmaster Tools, XML Sitemaps, and, if possible, from an internal database, such as an export of all product and category URLs on an eCommerce website. These can then be crawled in “ list mode ” separately, then added to your main list of URLs and deduplicated to produce a more comprehensive list of indexable URLs.\nSome URLs found via GA, XML sitemaps, and other non-crawl sources may not actually be “indexable.” These should be excluded. One strategy that works here is to combine and deduplicate all of the URL “lists,” and then perform a crawl in list mode. Once crawled, remove all URLs with robots meta or X-Robots noindex tags, as well as any URL returning error codes and those that are blocked by the robots.txt file, etc. At this point, you can safely add these URLs to the file containing indexable URLs from the crawl. Once again, deduplicate the list.\nCrawling roadblocks & new technologies Crawling very large websites First and foremost, you do not need to crawl every URL on the site. Be concerned with indexable content. This is not a technical SEO audit.\n{Expand for more about crawling very large websites}\nAvoid crawling unnecessary URLs\nSome of the things you can avoid crawling and adding to the content audit in many cases include:\nNoindexed or robots.txt-blocked URLs 4XX and 5XX errors Redirecting URLs and those that canonicalize to a different URL Images, CSS, JavaScript, and SWF files Segment the site into crawlable chunks\nYou can often get Screaming Frog to completely crawl a single directory at a time if the site is too large to crawl all at once.\nFilter out URL patterns you plan to remove from the index\nLet’s say you’re auditing a domain on WordPress and you notice early in the crawl that /tag/ pages are indexable. A quick site:domain.com inurl:tag search on Google tells you there are about 10 million of them. A quick look at Google Analytics confirms that URLs in the /tag/ directory are not responsible for very much revenue from organic search. It would be safe to say that the “Action” on these URLs should be “Remove” and the “Details” should read something like this: Remove /tag/ URLs from the indexed with a robots noindex,follow meta tag. More advice on this strategy can be found here .\nUpgrade your machine\nInstall additional RAM on your computer, which is used by Screaming Frog to hold data during the crawl. This has the added benefit of improving Excel performance, which can also be a major roadblock.\nYou can also install Screaming Frog on Amazon Web Server (AWS), as described in this post on iPullRank .\nTune up your tools\nScreaming Frog provides several ways for SEOs to get more out of the crawler. This includes adjusting the speed, max threads, search depth, query strings, timeouts, retries, and the amount of RAM available to the program. Leave at least 3GB off limits to the spider to avoid catastrophic freezing of the entire machine and loss of data. You can learn more about tuning up Screaming Frog here and here .\nTry other tools\nI’m convinced that there's a ton of wasted bandwidth on most content audit projects due to strategists releasing a crawler and allowing it to chew through an entire domain, whether the URLs are indexable or not. People run Screaming Frog without saving the crawl intermittently, without adding more RAM availability, without filtering out the nonsense, or using any of the crawl customization features available to them.\nThat said, sometimes SF just doesn’t get the job done. We also have a process specific to DeepCrawl , and have used Botify , as well as other tools. They each have their pros and cons. I still prefer Screaming Frog for crawling and URL Profiler for fetching metrics in most cases.\nCrawling dynamic mobile sites This refers to a specific type of mobile setup in which there are two code-bases –– one for mobile and one for desktop –– but only one URL. Thus, the content of a single URL may vary significantly depending on which type of device is visiting that URL. In such cases, you will essentially be performing two separate content audits. Proceed as usual for the desktop version. Below are instructions for crawling the mobile version.\n{Expand for more on crawling dynamic websites} Crawling a dynamic mobile site for a content audit will require changing the User-Agent of the crawler, as shown here under Screaming Frog’s “Configure ---> HTTP Header” menu:\nThe important thing to remember when working on mobile dynamic websites is that you're only taking an inventory of indexable URLs on one version of the site or the other. Once the two inventories are taken, you can then compare them to uncover any unintentional issues.\nSome examples of what this process can find in a technical SEO audit include situations in which titles, descriptions, canonical tags, robots meta, rel next/prev, and other important elements do not match between the two versions of the page. It's vital that the mobile and desktop version of each page have parity when it comes to these essentials.\nIt's easy for the mobile version of a historically desktop-first website to end up providing conflicting instructions to search engines because it's not often “automatically changed” when the desktop version changes. A good example here is a website I recently looked at with about 20 million URLs, all of which had the following title tag when loaded by a mobile user (including Google): BRAND NAME - MOBILE SITE. Imagine the consequences of that once a mobile-first algorithm truly rolls out.\nCrawling and rendering JavaScript One of the many technical issues SEOs have been increasingly dealing with over the last couple of years is the proliferation of websites built on JavaScript frameworks and libraries like React.js, Ember.js, and Angular.js.\n{Expand for more on crawling Javascript websites} Most crawlers have made a lot of progress lately when it comes to crawling and rendering JavaScript content. Now, it’s as easy as changing a few settings, as shown below with Screaming Frog.\nWhen crawling URLs with #! , use the “Old AJAX Crawling Scheme.” Otherwise, select “JavaScript” from the “Rendering” tab when configuring your Screaming Frog SEO Spider to crawl JavaScript websites.\nHow do you know if you’re dealing with a JavaScript website?\nFirst of all, most websites these days are going to be using some sort of JavaScript technology, though more often than not (so far) these will be rendered by the “client” (i.e., by your browser). An example would be the .js file that controls the behavior of a form or interactive tool.\nWhat we’re discussing here is when the JavaScript is used “server-side” and needs to be executed in order to render the page.\nJavaScript libraries and frameworks are used to develop single-page web apps and highly interactive websites. Below are a few different things that should alert you to this challenge:\nThe URLs contain #! (hashbangs). For example: example.com/page#!key=value (AJAX) Content-rich pages with only a few lines of code (and no iframes) when viewing the source code. What looks like server-side code in the meta tags instead of the actual content of the tag. For example:\nYou can also use the BuiltWith Technology Profiler or the Library Detector plugins for Chrome, which shows JavaScript libraries being used on a page in the address bar.\nNot all websites built primarily with JavaScript require special attention to crawl settings. Some websites use pre-rendering services like Brombone or Prerender.io to serve the crawler a fully rendered version of the page. Others use isomorphic JavaScript to accomplish the same thing.\nStep 2: Gather additional metrics Most crawlers will give you the URL and various on-page metrics and data, such as the titles, descriptions, meta tags, and word count. In addition to these, you’ll want to know about internal and external links, traffic, content uniqueness, and much more in order to make fully informed recommendations during the analysis portion of the content audit project.\nYour process may vary, but we generally try to pull in everything we need using as few sources as possible. URL Profiler is a great resource for this purpose, as it works well with Screaming Frog and integrates easily with all of the APIs we need.\nOnce the Screaming Frog scan is complete (only crawling indexable content) export the “Internal All” file, which can then be used as the seed list in URL Profiler (combined with any additional indexable URLs found outside of the crawl via GSC, GA, and elsewhere).\nThis is what my URL Profiler settings look for a typical content audit for a small- or medium-sized site. Also, under “Accounts” I have connected via API keys to Moz and SEMrush.\nOnce URL Profiler is finished, you should end up with something like this:\nScreaming Frog and URL Profiler: Between these two tools and the APIs they connect with, you may not need anything else at all in order to see the metrics below for every indexable URL on the domain.\nThe risk of getting analytics data from a third-party tool\nWe've noticed odd data mismatches and sampled data when using the method above on large, high-traffic websites. Our internal process involves exporting these reports directly from Google Analytics, sometimes incorporating Analytics Canvas to get the full, unsampled data from GA. Then VLookups are used in the spreadsheet to combine the data, with URL being the unique identifier.\nMetrics to pull for each URL:\nIndexed or not? If crawlers are set up properly, all URLs should be “indexable.” A non-indexed URL is often a sign of an uncrawled or low-quality page. Content uniqueness Copyscape, Siteliner, and now URL Profiler can provide this data. Traffic from organic search Typically 90 days Keep a consistent timeframe across all metrics. Revenue and/or conversions You could view this by “total,” or by segmenting to show only revenue from organic search on a per-page basis. Publish date If you can get this into Google Analytics as a custom dimension prior to fetching the GA data, it will help you discover stale content. Internal links Content audits provide the perfect opportunity to tighten up your internal linking strategy by ensuring the most important pages have the most internal links. External links These can come from Moz , SEMRush , and a variety of other tools, most of which integrate natively or via APIs with URL Profiler . Landing pages resulting in low time-on-site Take this one with a grain of salt. If visitors found what they want because the content was good, that’s not a bad metric. A better proxy for this would be scroll depth, but that would probably require setting up a scroll-tracking “event.” Landing pages resulting in Low Pages-Per-Visit Just like with Time-On-Site, sometimes visitors find what they’re looking for on a single page. This is often true for high-quality content. Response code Typically, only URLs that return a 200 (OK) response code are indexable. You may not require this metric in the final data if that's the case on your domain. Canonical tag Typically only URLs with a self-referencing rel=“canonical” tag should be considered “indexable.” You may not require this metric in the final data if that's the case on your domain. Page speed and mobile-friendliness Again, URL Profiler comes through with their Google PageSpeed Insights API integration . Before you begin analyzing the data, be sure to drastically improve your mental health and the performance of your machine by taking the opportunity to get rid of any data you don’t need. Here are a few things you might consider deleting right away (after making a copy of the full data set, of course).", "external_links": ["https://www.ithinkmedia.co.uk/ebay/", "http://support.urlprofiler.com/knowledge_base/categories/apis", "http://isomorphic.net/", "http://www.goinflow.com/", "http://www.seobythesea.com/2016/10/google-patents-context-vectors-improve-search/", "https://prerender.io/", "https://support.office.com/en-us/article/Filter-for-unique-values-or-remove-duplicate-values-ccf664b0-81d6-449b-bbe1-8daaec1e83c2", "https://www.semrush.com/features/backlinks/", "https://www.screamingfrog.co.uk/seo-spider/user-guide/configuration/", "https://chrome.google.com/webstore/detail/library-detector/cgaocdmhkmfnkdkbnckgmpopcbpaaejo?hl=en", "http://www.goinflow.com/do-you-really-need-to-crawl-all-3435198-pages/", "http://www.brombone.com/", "https://docs.google.com/a/moz.com/document/d/1SsCfG8xv9Z_32grCph2LrWo5xCh_feHIHelVR_-Wk9Q/edit?usp=sharing", "https://www.deepcrawl.com/", "http://www.seerinteractive.com/blog/screaming-frog-guide/", "http://analyticscanvas.com/", "http://www.goinflow.com/case-studies/?_sft_project-type=content-marketing", "http://support.urlprofiler.com/knowledge_base/topics/google-pagespeed-api", "https://www.botify.com/", "http://www.slideshare.net/randfish/fight-back-against-back/23-Training_Dataeg_good_search_resultsThis", "https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js", "https://chrome.google.com/webstore/detail/builtwith-technology-prof/dapjbgnjinbpoindlpdmhochffioedbn?hl=en", "https://www.mobilemoxie.com/blog/app-seo-news/understanding-mobile-first-indexing-23-long-term-impact-seo/", "https://en.wikipedia.org/wiki/Single-page_application", "http://ipullrank.com/how-to-run-screaming-frog-and-url-profiler-on-amazon-web-services/", "https://web.archive.org/web/20170321010341/https://moz.com/blog/content-audit-tutorial", "https://www.advancedwebranking.com/blog/how-to-track-scroll-depth-with-google-tag-manager/", "https://www.screamingfrog.co.uk/", "http://www.slideshare.net/EverettSizemore/cut-the-cruft-everett-sizemore-moztalk-denver-2016", "http://urlprofiler.com/features-content-audits/", "http://www.beaconfire-red.com/epic-stuff/how-crawl-large-websites-using-screaming-frog"], "published": "2017-03-22T10:04:00.000+02:00", "crawled": "2017-03-23T03:27:31.785+02:00", "highlightTitle": ""}