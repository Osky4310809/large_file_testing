{"organizations": [], "uuid": "b807c2cf98668d9120e2223a78c5777a11899857", "thread": {"social": {"gplus": {"shares": 16}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 15}, "facebook": {"likes": 350, "shares": 350, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "feeds.mashable.com", "main_image": "https://i0.wp.com/blog.goviral.co.id/wp-content/uploads/2017/03/49b3a5b4b6c848f3adeb039ad6d8e422.f6e11.jpg?resize=660%2C330", "site_section": "http://blog.goviral.co.id/feed/", "section_title": "Goviral Feed", "url": "http://feeds.mashable.com/~r/mashable/tech/~3/SUQ-ENJFHhY/", "country": "ID", "domain_rank": 360, "title": "Video of Tesla crash shows exactly why Autopilot isn’t true self-driving tech", "performance_score": 3, "site": "mashable.com", "participants_count": 1, "title_full": "Video of Tesla crash shows exactly why Autopilot isn’t true self-driving tech", "spam_score": 0.0, "site_type": "blogs", "published": "2017-03-04T15:13:00.000+02:00", "replies_count": 0, "uuid": "b807c2cf98668d9120e2223a78c5777a11899857"}, "author": "Dianike Saraswati", "url": "http://feeds.mashable.com/~r/mashable/tech/~3/SUQ-ENJFHhY/", "ord_in_thread": 0, "title": "Video of Tesla crash shows exactly why Autopilot isn’t true self-driving tech", "locations": [], "entities": {"persons": [{"name": "tesla", "sentiment": "negative"}, {"name": "electrek", "sentiment": "none"}], "locations": [{"name": "tesla", "sentiment": "none"}], "organizations": [{"name": "nhtsa", "sentiment": "none"}, {"name": "tesla motors", "sentiment": "none"}, {"name": "national highway traffic safety administration", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "Home / Science & Tech / Video of Tesla crash shows exactly why Autopilot isn’t true self-driving tech Video of Tesla crash shows exactly why Autopilot isn’t true self-driving tech Reminder: Autopilot doesn’t mean your car can totally drive itself. \nImage: tyler essary/mashable 2017-03-03 22:01:46 UTC Listen up, Tesla fans. \nYour favorite carmaker’s Autopilot is a super-smart marriage of high tech hardware and software that provides one of the best semi-autonomous driving experiences currently on the road. \nThat said: It is not a fully self-driving system and shouldn’t be used as such. \nA video posted to the Tesla Motors subreddit and spotted by Electrek shows exactly what can happen when a Tesla Model S driver puts too much trust in his car’s Autopilot. It’s a reminder that a human driver is always needed in a Tesla, even when the Autopilot system is engaged. \nThe accident first came to light when a redditor posted pictures of his thrashed Model S. He claimed his first-generation Autopilot never alerted him to take manual control of the car — the system is supposed to cue drivers to take over when the car can’t handle road conditions — before it “misread the road” and sideswiped the highway barrier. He walked away from the crash with some minor bruises. \nA few days later, the video of the alleged incident was posted to the subreddit. The footage, taken by the dashcam of a car directly behind the Model S, shows a poorly-marked construction zone playing havoc with the Autopilot system, running the car into the lane barrier. You can watch for yourself here. \nOther redditors were quick to point out the original poster never mentioned a construction zone when he shared the pics, let alone the shoddy road markings that made the particular stretch of highway tricky for even attentive human drivers. Most commenters agreed on one thing: the driver wasn’t paying enough attention to the road — and was therefore using Autopilot the wrong way. \nThey’re right. As cool as it is that you can flip on Autopilot on the highway and let the car handle most of the work, it’s not safe to give it free rein, or even take your hands off the wheel. \nTesla’s first generation Autopilot system has Level 2 autonomy, which means the car can handle some of the driving responsibilities on its own, mostly for breaking and keeping the car in its lane. But a human still needs to be in the driver’s seat keeping an eye on the road, ready to take the wheel when the machine can’t handle the conditions. \nSome groups, like the German government, have argued that “Autopilot” is a misnomer at best, and potentially fatal false advertising at worst. The system came under fire after the first self-driving fatality last year, when a distracted driver’s Model S drove underneath a white tractor trailer. A National Highway Traffic Safety Administration (NHTSA) investigation, however, cleared Tesla of any liability, deeming Autopilot safe if used correctly. \nTesla has bigger plans for the system with Enhanced Autopilot. Updated sensor hardware now available in new models, along with incremental software updates that will be released throughout this year, will purportedly bring the system to Level 5 autonomy. That means full-on self-driving cars that can handle any and all road conditions just as well as a human driver can — potentially better. \nFor now, though, the full Enhanced Autopilot and Level 5 autonomy isn’t here — so until it is, remember that your “self-driving” cars still need your full attention.", "external_links": [], "published": "2017-03-04T15:13:00.000+02:00", "crawled": "2017-03-04T11:23:40.725+02:00", "highlightTitle": ""}