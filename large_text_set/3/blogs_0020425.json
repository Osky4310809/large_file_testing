{"organizations": [], "uuid": "f940523ea80e2c3ae5acb0edaa70f1d6e66265f8", "thread": {"social": {"gplus": {"shares": 98}, "pinterest": {"shares": 12}, "vk": {"shares": 0}, "linkedin": {"shares": 267}, "facebook": {"likes": 254, "shares": 254, "comments": 0}, "stumbledupon": {"shares": 50}}, "site_full": "moz.com", "main_image": "https://d1avok0lzls2w.cloudfront.net/uploads/og_image/58d1eedddb3491.09385308.png", "site_section": "", "section_title": "", "url": "https://moz.com/blog/content-audit", "country": "US", "domain_rank": 1643, "title": "How to Do a Content Audit [Updated for 2017] - Moz", "performance_score": 2, "site": "moz.com", "participants_count": 0, "title_full": "How to Do a Content Audit [Updated for 2017] - Moz", "spam_score": 0.166, "site_type": "blogs", "published": "2017-03-22T07:04:00.000+02:00", "replies_count": 0, "uuid": "f940523ea80e2c3ae5acb0edaa70f1d6e66265f8"}, "author": "Everett Sizemore March", "url": "https://moz.com/blog/content-audit", "ord_in_thread": 0, "title": "How to Do a Content Audit [Updated for 2017] - Moz", "locations": [], "entities": {"persons": [{"name": "botify", "sentiment": "none"}, {"name": "siteliner", "sentiment": "none"}, {"name": "brombone", "sentiment": "none"}, {"name": "bill slawski", "sentiment": "none"}], "locations": [{"name": "ga", "sentiment": "none"}], "organizations": [{"name": "ajax", "sentiment": "none"}, {"name": "gsc", "sentiment": "none"}, {"name": "swf", "sentiment": "none"}, {"name": "ga", "sentiment": "none"}, {"name": "css", "sentiment": "none"}, {"name": "google sheets", "sentiment": "none"}, {"name": "seo", "sentiment": "none"}, {"name": "amazon", "sentiment": "none"}, {"name": "google analytics", "sentiment": "none"}, {"name": "google", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "\nTaking an inventory of all content, and related metrics, begins with crawling the site. \nOne difference between crawling for content audits and technical audits: \nTechnical SEO audit crawls are concerned with all crawlable content (among other things). \nContent audit crawls for the purpose of SEO are concerned with all indexable content. \n{Expand for more on crawlable vs. indexable content} \nThe URL in the image below should be considered non-indexable. Even if it isn’t blocked in the robots.txt file, with a robots meta tag, or an X-robots header response –– even if it is frequently crawled by Google and shows up as a URL in Google Analytics and Search Console –– the rel =”canonical” tag shown below essentially acts like a 301 redirect, telling Google not to display the non-canonical URL in search results and to apply all ranking calculations to the canonical version. In other words, not to “index” it. \nI'm not sure “index” is the best word, though. To “display” or “return” in the SERPs is a better way of describing it, as Google surely records canonicalized URL variants somewhere, and advanced site: queries seem to show them in a way that is consistent with the \"supplemental index\" of yesteryear. But that's another post, more suitably written by a brighter mind like Bill Slawski. A URL with a query string that canonicalizes to a version without the query string can be considered “not indexable.” \nA content audit can safely ignore these types of situations, which could mean drastically reducing the amount of time and memory taken up by a crawl. \nTechnical SEO audits, on the other hand, should be concerned with every URL a crawler can find. Non-indexable URLs can reveal a lot of technical issues, from spider traps (e.g. never-ending empty pagination, infinite loops via redirect or canonical tag) to crawl budget optimization (e.g. How many facets/filters deep to allow crawling? 5? 6? 7?) and more. \nIt is for this reason that trying to combine a technical SEO audit with a content audit often turns into a giant mess , though an efficient idea in theory. When dealing with a lot of data, I find it easier to focus on one or the other: all crawlable URLs, or all indexable URLs. \nOrphaned pages (i.e., with no internal links / navigation path) sometimes don’t turn up in technical SEO audits if the crawler had no way to find them. Content audits should discover any indexable content, whether it is linked to internally or not. Side note: A good tech audit would do this, too. \nIdentifying URLs that should be indexed but are not is something that typically happens during technical SEO audits. \nHowever, if you're having trouble getting deep pages indexed when they should be, content audits may help determine how to optimize crawl budget and herd bots more efficiently into those important, deep pages. Also, many times Google chooses not to display/index a URL in the SERPs due to poor content quality (i.e., thin or duplicate). \nAll of this is changing rapidly, though . URLs as the unique identifier in Google’s index are probably going away . Yes, we’ll still have URLs, but not everything requires them. So far, the word “content” and URL has been mostly interchangeable. But some URLs contain an entire application’s worth of content. How to do a content audit in that world is something we’ll have to figure out soon, but only after Google figures out how to organize the web’s information in that same world. From the looks of things, we still have a year or two. \nUntil then, the process below should handle most situations. Step 1: Crawl all indexable URLs \nA good place to start on most websites is a full Screaming Frog crawl. However, some indexable content might be missed this way. It is not recommended that you rely on a crawler as the source for all indexable URLs. \nIn addition to the crawler, collect URLs from Google Analytics, Google Webmaster Tools, XML Sitemaps, and, if possible, from an internal database, such as an export of all product and category URLs on an eCommerce website. These can then be crawled in “ list mode ” separately, then added to your main list of URLs and deduplicated to produce a more comprehensive list of indexable URLs. \nSome URLs found via GA, XML sitemaps, and other non-crawl sources may not actually be “indexable.” These should be excluded. One strategy that works here is to combine and deduplicate all of the URL “lists,” and then perform a crawl in list mode. Once crawled, remove all URLs with robots meta or X-Robots noindex tags, as well as any URL returning error codes and those that are blocked by the robots.txt file, etc. At this point, you can safely add these URLs to the file containing indexable URLs from the crawl. Once again, deduplicate the list. Crawling roadblocks & new technologies Crawling very large websites \nFirst and foremost, you do not need to crawl every URL on the site. Be concerned with indexable content. This is not a technical SEO audit. \n{Expand for more about crawling very large websites} \nAvoid crawling unnecessary URLs \nSome of the things you can avoid crawling and adding to the content audit in many cases include: Noindexed or robots.txt-blocked URLs Redirecting URLs and those that canonicalize to a different URL Images, CSS, JavaScript, and SWF files \nSegment the site into crawlable chunks \nYou can often get Screaming Frog to completely crawl a single directory at a time if the site is too large to crawl all at once. \nFilter out URL patterns you plan to remove from the index \nLet’s say you’re auditing a domain on WordPress and you notice early in the crawl that /tag/ pages are indexable. A quick site:domain.com inurl:tag search on Google tells you there are about 10 million of them. A quick look at Google Analytics confirms that URLs in the /tag/ directory are not responsible for very much revenue from organic search. It would be safe to say that the “Action” on these URLs should be “Remove” and the “Details” should read something like this: Remove /tag/ URLs from the indexed with a robots noindex,follow meta tag. More advice on this strategy can be found here . \nUpgrade your machine \nInstall additional RAM on your computer, which is used by Screaming Frog to hold data during the crawl. This has the added benefit of improving Excel performance, which can also be a major roadblock. \nYou can also install Screaming Frog on Amazon Web Server (AWS), as described in this post on iPullRank . \nTune up your tools \nScreaming Frog provides several ways for SEOs to get more out of the crawler. This includes adjusting the speed, max threads, search depth, query strings, timeouts, retries, and the amount of RAM available to the program. Leave at least 3GB off limits to the spider to avoid catastrophic freezing of the entire machine and loss of data. You can learn more about tuning up Screaming Frog here and here . \nTry other tools \nI’m convinced that there's a ton of wasted bandwidth on most content audit projects due to strategists releasing a crawler and allowing it to chew through an entire domain, whether the URLs are indexable or not. People run Screaming Frog without saving the crawl intermittently, without adding more RAM availability, without filtering out the nonsense, or using any of the crawl customization features available to them. \nThat said, sometimes SF just doesn’t get the job done. We also have a process specific to DeepCrawl , and have used Botify , as well as other tools. They each have their pros and cons. I still prefer Screaming Frog for crawling and URL Profiler for fetching metrics in most cases. Crawling dynamic mobile sites \nThis refers to a specific type of mobile setup in which there are two code-bases –– one for mobile and one for desktop –– but only one URL. Thus, the content of a single URL may vary significantly depending on which type of device is visiting that URL. In such cases, you will essentially be performing two separate content audits. Proceed as usual for the desktop version. Below are instructions for crawling the mobile version. {Expand for more on crawling dynamic websites} \nCrawling a dynamic mobile site for a content audit will require changing the User-Agent of the crawler, as shown here under Screaming Frog’s “Configure ---> HTTP Header” menu: \nThe important thing to remember when working on mobile dynamic websites is that you're only taking an inventory of indexable URLs on one version of the site or the other. Once the two inventories are taken, you can then compare them to uncover any unintentional issues. \nSome examples of what this process can find in a technical SEO audit include situations in which titles, descriptions, canonical tags, robots meta, rel next/prev, and other important elements do not match between the two versions of the page. It's vital that the mobile and desktop version of each page have parity when it comes to these essentials. \nIt's easy for the mobile version of a historically desktop-first website to end up providing conflicting instructions to search engines because it's not often “automatically changed” when the desktop version changes. A good example here is a website I recently looked at with about 20 million URLs, all of which had the following title tag when loaded by a mobile user (including Google): BRAND NAME - MOBILE SITE. Imagine the consequences of that once a mobile-first algorithm truly rolls out. Crawling and rendering JavaScript \nOne of the many technical issues SEOs have been increasingly dealing with over the last couple of years is the proliferation of websites built on JavaScript frameworks and libraries like React.js, Ember.js, and Angular.js. {Expand for more on crawling Javascript websites} \nMost crawlers have made a lot of progress lately when it comes to crawling and rendering JavaScript content. Now, it’s as easy as changing a few settings, as shown below with Screaming Frog. When crawling URLs with #! , use the “Old AJAX Crawling Scheme.” Otherwise, select “JavaScript” from the “Rendering” tab when configuring your Screaming Frog SEO Spider to crawl JavaScript websites. \nHow do you know if you’re dealing with a JavaScript website? \nFirst of all, most websites these days are going to be using some sort of JavaScript technology, though more often than not (so far) these will be rendered by the “client” (i.e., by your browser). An example would be the .js file that controls the behavior of a form or interactive tool. \nWhat we’re discussing here is when the JavaScript is used “server-side” and needs to be executed in order to render the page. \nJavaScript libraries and frameworks are used to develop single-page web apps and highly interactive websites. Below are a few different things that should alert you to this challenge: The URLs contain #! (hashbangs). For example: example.com/page#!key=value (AJAX) Content-rich pages with only a few lines of code (and no iframes) when viewing the source code. What looks like server-side code in the meta tags instead of the actual content of the tag. For example: \nYou can also use the BuiltWith Technology Profiler or the Library Detector plugins for Chrome, which shows JavaScript libraries being used on a page in the address bar. \nNot all websites built primarily with JavaScript require special attention to crawl settings. Some websites use pre-rendering services like Brombone or Prerender.io to serve the crawler a fully rendered version of the page. Others use isomorphic JavaScript to accomplish the same thing. Step 2: Gather additional metrics \nMost crawlers will give you the URL and various on-page metrics and data, such as the titles, descriptions, meta tags, and word count. In addition to these, you’ll want to know about internal and external links, traffic, content uniqueness, and much more in order to make fully informed recommendations during the analysis portion of the content audit project. \nYour process may vary, but we generally try to pull in everything we need using as few sources as possible. URL Profiler is a great resource for this purpose, as it works well with Screaming Frog and integrates easily with all of the APIs we need. \nOnce the Screaming Frog scan is complete (only crawling indexable content) export the “Internal All” file, which can then be used as the seed list in URL Profiler (combined with any additional indexable URLs found outside of the crawl via GSC, GA, and elsewhere). This is what my URL Profiler settings look for a typical content audit for a small- or medium-sized site. Also, under “Accounts” I have connected via API keys to Moz and SEMrush. \nOnce URL Profiler is finished, you should end up with something like this: Screaming Frog and URL Profiler: Between these two tools and the APIs they connect with, you may not need anything else at all in order to see the metrics below for every indexable URL on the domain. \nThe risk of getting analytics data from a third-party tool \nWe've noticed odd data mismatches and sampled data when using the method above on large, high-traffic websites. Our internal process involves exporting these reports directly from Google Analytics, sometimes incorporating Analytics Canvas to get the full, unsampled data from GA. Then VLookups are used in the spreadsheet to combine the data, with URL being the unique identifier. \nMetrics to pull for each URL: Indexed or not? If crawlers are set up properly, all URLs should be “indexable.” A non-indexed URL is often a sign of an uncrawled or low-quality page. Content uniqueness Copyscape, Siteliner, and now URL Profiler can provide this data. Traffic from organic search Keep a consistent timeframe across all metrics. Revenue and/or conversions You could view this by “total,” or by segmenting to show only revenue from organic search on a per-page basis. Publish date If you can get this into Google Analytics as a custom dimension prior to fetching the GA data, it will help you discover stale content. Internal links Content audits provide the perfect opportunity to tighten up your internal linking strategy by ensuring the most important pages have the most internal links. External links These can come from Moz , SEMRush , and a variety of other tools, most of which integrate natively or via APIs with URL Profiler . Landing pages resulting in low time-on-site Take this one with a grain of salt. If visitors found what they want because the content was good, that’s not a bad metric. A better proxy for this would be scroll depth, but that would probably require setting up a scroll-tracking “event.” Landing pages resulting in Low Pages-Per-Visit Just like with Time-On-Site, sometimes visitors find what they’re looking for on a single page. This is often true for high-quality content. Response code Typically, only URLs that return a 200 (OK) response code are indexable. You may not require this metric in the final data if that's the case on your domain. Canonical tag Typically only URLs with a self-referencing rel=“canonical” tag should be considered “indexable.” You may not require this metric in the final data if that's the case on your domain. Page speed and mobile-friendliness Again, URL Profiler comes through with their Google PageSpeed Insights API integration . \nBefore you begin analyzing the data, be sure to drastically improve your mental health and the performance of your machine by taking the opportunity to get rid of any data you don’t need. Here are a few things you might consider deleting right away (after making a copy of the full data set, of course). Things you don’t need when analyzing the data {Expand for more on removing unnecessary data} \nURL Profiler and Screaming Frog tabs Just keep the “combined data” tab and immediately cut the amount of data in the spreadsheet by about half. \nContent Type Filtering by Content Type (e.g., text/html, image, PDF, CSS, JavaScript) and removing any URL that is of no concern in your content audit is a good way to speed up the process. \nTechnically speaking, images can be indexable content. However, I prefer to deal with them separately for now. \nFiltering unnecessary file types out like I've done in the screenshot above improves focus, but doesn’t improve performance very much. A better option would be to first select the file types you don’t want, apply the filter, delete the rows you don’t want, and then go back to the filter options and “(Select All).” \nOnce you have only the content types you want, it may now be possible to simply delete the entire Content Type column. \nStatus Code and Status You only need one or the other. I prefer to keep the Code, and delete the Status column. \nLength and Pixels You only need one or the other. I prefer to keep the Pixels, and delete the Length column. This applies to all Title and Meta Description columns. \nMeta Keywords Delete the columns. If those cells have content, consider removing that tag from the site. \nDNS Safe URL, Path, Domain, Root, and TLD You should really only be working on a single top-level domain. Content audits for subdomains should probably be done separately. Thus, these columns can be deleted in most cases. \nDuplicate Columns You should have two columns for the URL (The “Address” in column A from URL Profiler, and the “URL” column from Screaming Frog). Similarly, there may also be two columns each for HTTP Status and Status Code. It depends on the settings selected in both tools, but there are sure to be some overlaps, which can be removed to reduce the file size, enhance focus, and speed up the process. \nBlank Columns Keep the filter tool active and go through each column. Those with only blank cells can be deleted. The example below shows that column BK (Robots HTTP Header) can be removed from the spreadsheet. \n[You can save a lot of headspace by hiding or removing blank columns.] \nSingle-Value Columns If the column contains only one value, it can usually be removed. The screenshot below shows our non-secure site does not have any HTTPS URLs, as expected. I can now remove the column. Also, I guess it’s probably time I get that HTTPS migration project scheduled. \nHopefully by now you've made a significant dent in reducing the overall size of the file and time it takes to apply formatting and formula changes to the spreadsheet. It’s time to start diving into the data. The analysis & recommendations phase \nHere's where the fun really begins. In a large organization, it's tempting to have a junior SEO do all of the data-gathering up to this point. I find it useful to perform the crawl myself, as the process can be highly informative. Step 3: Put it all into a dashboard \nEven after removing unnecessary data, performance could still be a major issue, especially if working in Google Sheets. I prefer to do all of this in Excel, and only upload into Google Sheets once it's ready for the client. If Excel is running slow, consider splitting up the URLs by directory or some other factor in order to work with multiple, smaller spreadsheets. \nCreating a dashboard can be as easy as adding two columns to the spreadsheet. The first new column, “Action,” should be limited to three options, as shown below. This makes filtering and sorting data much easier. The “Details” column can contain freeform text to provide more detailed instructions for implementation. \nUse Data Validation and a drop-down selector to limit Action options. Step 4: Work the content audit dashboard \nAll of the data you need should now be right in front of you. This step can’t be turned into a repeatable process for every content audit. From here on the actual step-by-step process becomes much more open to interpretation and your own experience. You may do some of them and not others. You may do them a little differently. That's all fine, as long as you're working toward the goal of determining what to do, if anything, for each piece of content on the website. \nA good place to start would be to look for any content-related issues that might cause an algorithmic filter or manual penalty to be applied, thereby dragging down your rankings. Causes of content-related penalties \nThese typically fall under three major categories: quality, duplication, and relevancy . Each category can be further broken down into a variety of issues, which are detailed below. ", "external_links": [], "published": "2017-03-22T07:04:00.000+02:00", "crawled": "2017-03-22T09:21:10.303+02:00", "highlightTitle": ""}