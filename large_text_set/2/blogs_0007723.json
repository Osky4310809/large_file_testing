{"organizations": [], "uuid": "38fa718ca97b3310d91b299c46d5251501352778", "thread": {"social": {"gplus": {"shares": 5}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 216}, "facebook": {"likes": 130, "shares": 130, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "blogs.microsoft.com", "main_image": "https://mscorpmedia.azureedge.net/mscorpmedia/2017/02/1W0C9429_170202A-next-feature.jpg", "site_section": "http://blogs.microsoft.com/next/feed/", "section_title": "Next at Microsoft", "url": "https://blogs.microsoft.com/next/2017/02/07/microsoft-cognitive-services-push-gains-momentum/", "country": "IE", "title": "Microsoft Cognitive Services push gains momentum", "performance_score": 1, "site": "microsoft.com", "participants_count": 1, "title_full": "Microsoft Cognitive Services push gains momentum", "spam_score": 0.0, "site_type": "blogs", "published": "2017-02-07T21:00:00.000+02:00", "replies_count": 0, "uuid": "38fa718ca97b3310d91b299c46d5251501352778"}, "author": "John Roach", "url": "https://blogs.microsoft.com/next/2017/02/07/microsoft-cognitive-services-push-gains-momentum/", "ord_in_thread": 0, "title": "Microsoft Cognitive Services push gains momentum", "locations": [], "entities": {"persons": [{"name": "andrew shuman", "sentiment": "none"}, {"name": "alexander mejia", "sentiment": "none"}, {"name": "bing", "sentiment": "none"}, {"name": "seltzer", "sentiment": "none"}, {"name": "hussein salama", "sentiment": "none"}, {"name": "mejia", "sentiment": "none"}, {"name": "mike seltzer", "sentiment": "none"}, {"name": "john roach", "sentiment": "none"}], "locations": [{"name": "egypt", "sentiment": "none"}, {"name": "cairo", "sentiment": "none"}, {"name": "washington", "sentiment": "none"}, {"name": "redmond", "sentiment": "none"}, {"name": "cortana", "sentiment": "none"}], "organizations": [{"name": "microsoft cognitive services", "sentiment": "none"}, {"name": "custom speech service", "sentiment": "none"}, {"name": "human interact", "sentiment": "none"}, {"name": "computer vision api", "sentiment": "none"}, {"name": "speech and dialog research group", "sentiment": "none"}, {"name": "bing speech api", "sentiment": "none"}, {"name": "microsoft", "sentiment": "none"}, {"name": "microsoft cognitive services learn", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "Microsoft Cognitive Services push gains momentum Posted 56 \nThe machine-learned smarts that enable Microsoft’s Skype Translator, Bing and Cortana to accomplish tasks such as translating conversations, compiling knowledge and understanding the intent of spoken words are increasingly finding their way into third-party applications that people use every day. \nThese advances in the democratization of artificial intelligence are coming in part from Microsoft Cognitive Services , a collection of 25 tools that allow developers to add features such as emotion and sentiment detection, vision and speech recognition, and language understanding to their applications with zero expertise in machine learning. \n“Cognitive Services is about taking all of the machine learning and AI smarts that we have in this company and exposing them to developers through easy-to-use APIs, so that they don’t have to invent the technology themselves,” said Mike Seltzer , a principal researcher in the Speech and Dialog Research Group at Microsoft’s research lab in Redmond, Washington. \n“In most cases, it takes a ton of time, a ton of data, a ton of expertise, and a ton of compute to build a state-of-the-art machine-learned model,” he explained. \nTake one of the tools that deals with speech recognition, for example. Seltzer and his colleagues have spent more than a decade developing algorithms that enable Microsoft’s speech recognition technology to perform robustly in noisy environments as well as with the jargons, dialects and accents of specific user groups and settings. \nThe same flexible technology is now available to developers of third-party applications via the Custom Speech Service , a Cognitive Service that Microsoft released to public preview on Tuesday. \nTwo other Cognitive Services, the Content Moderator and Bing Speech API , will be moving to general availability next month, the company noted. The Content Moderator allows users to quarantine and review data such as images, text or videos to filter out unwanted material, such as potentially offensive language or pictures. The Bing Speech API converts audio into text, understands intent and converts text back to speech. Andrew Shuman, corporate vice president, Microsoft AI and Research. \nCognitive Services that enable developers to apply intelligence to visual data such as pictures and video are being used by customers to enhance their services. For example, business intelligence company Prism Skylabs used the Computer Vision API in its Prism Vision application, which helps organizations search through closed-circuit and security camera footage for specific events, items and people. \nThe entire collection of Cognitive Services stems from a drive within Microsoft to make its artificial intelligence and machine learning expertise widely accessible to the development community to create delightful and empowering experiences for end users, said Andrew Shuman , corporate vice president of products for Microsoft’s AI and Research organization. \n“Being able to have software now that observes people, listens, reacts and is knowledgeable about the physical world around them provides an excellent breakthrough in terms of making interfaces more human, more natural, more easy to understand and thus far more impactful in lots of different scenarios,” he said \n“This era that we are coming into is really an era of enhancing and bringing about more computer capabilities for more people in more interesting ways.” \nStorytelling experience Take Alexander Mejia, for example. Growing up, he always rushed to try the latest games with the latest graphics and technological innovations, chasing the buzz that comes with better sounds and resolution and new ways to convert bodily twitches into action on the screen. \nIn recent years, while working as a creative director in the gaming industry, the buzz from new experiences fizzled – doubling of computing power failed to result in a doubling of gaming excitement. “What is the next thing?” he asked. “What is the technology leap that is going to allow for new experiences that will wow the gamers?” \nThe questioning led to a demonstration of the latest generation of virtual reality technology. He strapped on the headgear and was taken for a wild ride on a roller coaster. The adrenaline rush returned. The experience, he said, was visceral. \n“You believe that things are real when in the virtual world,” he said. “What would happen if we put a person in front you? Would you try to talk to him?” \nThe idea blossomed to a business plan. Mejia founded his own company, Human Interact , to develop virtual reality storytelling experiences. The company’s premier title, Starship Commander, provides players control over the narrative as they zip around space at faster-than-light speed and speak with virtual characters at every turn. \nTo achieve realistic, fast-paced action, Mejia and his colleagues required accurate and responsive speech recognition. \n“You’ve got to make it so that anytime anybody says anything, [the speech recognition engine] is going to understand them and run them down the right path in the script,” he explained. “And that,” he added, “is the magic of Microsoft Cognitive Services.” \nCreating a custom speech model Modern speech recognition technology hinges on machine-learned statistical models that harness the power of cloud computing and massive troves of data to convert snippets of sound into text that is an accurate transcription of the spoken words. \nAn acoustic model, for example, is a classifier that labels short snippets of audio as one of a number phonemes, or sound units, in a given language. The labels are combined with those from the neighboring snippets to predict what word in the target language is being spoken, Seltzer explained. That prediction is guided by a dictionary that contains every word in the target language broken down to its phonemes. \nMeanwhile, a language model further refines the prediction by weighing how common every predicted word is in the target language. When the recognizer grapples with similar sounding words, the higher probability goes to the more common word. These models also consider context to make more robust predictions. “If the previous words are ‘The player caught the,’” explained Seltzer, “then ‘ball’ is going to be more likely than ‘fall.’” \nThe acoustic model that powers Microsoft’s state-of-the-art speech recognition engine is a deep neural network, a classifier inspired by theories about how pattern recognition occurs in the human brain. The model is trained on thousands of hours of audio using advanced algorithms that run in the cloud. \nMicrosoft’s speech recognition system recently hit a milestone when it recognized words in a conversation as well as a person . The milestone was achieved on a standardized test, or benchmark, that has been used by researchers in academia and industry for more than 20 years. \n“Now, if you take that same system and put it in a noisy factory and it had never seen noisy factory speech, it would not do a good job,” Seltzer said. “That is where the Custom Speech Service comes in.” \nThe service allows a developer to customize the acoustic and language models to the sounds of the noisy factory floor and the jargon of factory workers. The acoustic model, for example, can be trained to recognize speech amid the din of hydraulics and drills and the language model updated to give priority weighting to jargon specific to the factory, such as nuts, bolts and car parts. \nBeneath the hood, the Custom Speech Service leverages an algorithm that shifts Microsoft’s existing speech recognizer to the developer-supplied data. By starting from models that have been trained on massive troves of data, the amount of application-specific data required is greatly reduced. In cases where the developer’s data is insufficient, the recognizer falls back on the existing models. \n“The basic idea is that the more focused the systems can be, the better they will perform,” Seltzer said. “The job of the Custom Speech Service is to let you focus the system on the data that you care about.” \nCustomized for virtual reality Starship Commander, the premier title from Human Interact, takes place in a science fiction world that contains invented words and place names. When Mejia trained the Custom Speech Service on these keywords and phrases, he found the system made half as many errors as the open-source speech-to-text software he used to build an early prototype of the virtual reality experience. \nMejia then turned to Microsoft’s Language Understanding Service to address another concern – understanding the intent of what the gamers say. \n“There are a lot of different ways to say ‘let’s go,’” he explained. “There is, ‘let’s go; autopilot; get me out of here; let’s go faster than light; engage the hyper-drive.’ These are all different things people say to get going in our game, especially in the heat of the moment, because sometimes you don’t have very much time before something bad happens.” \nThe Language Understanding Service, which is currently in public preview, allows developers to train a classifier in a machine-learned model to understand the intent of natural language by uploading a subset of the type of things users might utter and tagging those utterances to an intention. \nOn the backend, the service harnesses more than a decade of research on how to train classifiers with a limited dataset, explained Hussein Salama , director of Microsoft’s Advanced Technology Lab in Cairo, Egypt, which is leading the development of the service. \n“Usually one needs an expert on machine learning to select the right technology and provide the right sets of data and to train the classifiers and then to evaluate them,” he said. “With the Language Understanding Service, we have simplified this. Provide a few utterances, a few examples of phrases with the intent, and then the Language Understanding Service can start training a model with good accuracy for that intent.” \nFor Starship Commander, the customization worked seamlessly, learning from the examples how to infer intent from natural language commands that were not part of the training data. “It’s shockingly scary how good it understands things that you never even trained it for,” Mejia said. “It is an AI.” \nRelated information: More information about Microsoft Cognitive Services Learn about Microsoft’s vision to democratize AI Check out the Microsoft Cognitive Toolkit \nJohn Roach writes about Microsoft research and innovation. Follow him on Twitter .", "external_links": [], "published": "2017-02-07T21:00:00.000+02:00", "crawled": "2017-02-08T02:00:05.973+02:00", "highlightTitle": ""}