{"organizations": [], "uuid": "6bd39030b8aafb3080fcc0bddcf62563edd1b719", "thread": {"social": {"gplus": {"shares": 12}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 20}, "facebook": {"likes": 128, "shares": 128, "comments": 3}, "stumbledupon": {"shares": 0}}, "site_full": "techcrunch.com", "main_image": "https://tctechcrunch2011.files.wordpress.com/2017/01/getty-cloud-computing-concept-image.jpg?w=764&amp;h=400&amp;crop=1", "site_section": "http://feedproxy.google.com/TechCrunch", "section_title": "TechCrunch", "url": "https://techcrunch.com/2017/03/02/aws-cloudsplains-what-happend-to-s3-storage-on-monday/", "country": "US", "domain_rank": 697, "title": "AWS cloudsplains what happend to S3 storage on Monday", "performance_score": 1, "site": "techcrunch.com", "participants_count": 1, "title_full": "AWS cloudsplains what happend to S3 storage on Monday", "spam_score": 0.0, "site_type": "blogs", "published": "2017-03-03T01:53:00.000+02:00", "replies_count": 0, "uuid": "6bd39030b8aafb3080fcc0bddcf62563edd1b719"}, "author": "Ron Miller", "url": "https://techcrunch.com/2017/03/02/aws-cloudsplains-what-happend-to-s3-storage-on-monday/", "ord_in_thread": 0, "title": "AWS cloudsplains what happend to S3 storage on Monday", "locations": [], "entities": {"persons": [{"name": "colin anderson", "sentiment": "none"}, {"name": "joe", "sentiment": "none"}], "locations": [{"name": "amazon", "sentiment": "none"}, {"name": "northern virginia", "sentiment": "none"}], "organizations": [{"name": "nasa", "sentiment": "negative"}, {"name": "aws", "sentiment": "none"}, {"name": "getty images", "sentiment": "none"}, {"name": "amazon", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "AWS cloudsplains what happend to S3 storage on Monday Posted NASA released a ton of software for free and here’s some you should try \nAWS took a lot of heat when the its S3 storage component went down for several hours on Monday, and rightly so, but today they published a post-mortem explained exactly what happened complete with technical details and how they plan to prevent a similar event from occurring again in the future. \nAt the core of the problem was unsurprisingly human error. Some poor engineer, we’ll call him Joe, was tasked with entering a command to shut down some storage sub-systems. On a typical day this doesn’t cause any issue whatsoever. It’s a routine kind of task, but on Monday something went terribly wrong. \nJoe was an authorized user, and he entered the command according to procedure based on what Amazon calls “an established playbook.” The problem was that Joe was supposed to issue a command to take down a small number of servers on an S3 sub-system, but he made a mistake, and instead of taking down just that small set of servers, Joe took down a much larger set of servers. \nIn layman’s terms, that’s when all hell broke loose. \nAmazon explains it much more technically, but suffice to say that error had a cascading impact on the S3 storage in the Northern Virginia datacenter. To make a long story short, Joe’s error took down some crucial underlying sub-systems, which removed a significant amount of storage capacity, which caused the systems to restart. As this happened, S3 couldn’t service requests, which caused even AWS’s own dashboard to go down (which is, you know, kind of embarrassing). \nBy now, the outside world started to feel the impact and your favorite websites, apps and cloud services were beginning to behave in a wonky fashion. Related Articles The day Amazon S3 storage stood still Amazon AWS S3 outage is breaking things for a lot of websites and apps Why AWS has such a big lead in the cloud WTF is cloud computing? \nAs the afternoon wore on, the company was working feverishly to get the systems back online, but the size of the systems was working against them. When the system shut down, something that AWS says it hasn’t had to do in many years, it became a victim of its own success. S3 capacity had grown to such an extent in the affected datacenter that when they restarted, running all of the safety checks and validating the integrity of the underlying metadata took a might bit longer than they expected. \nTo reduce the prospect of a similar human error in the future, the company is making some changes. In their words, “We have modified this tool to remove capacity more slowly and added safeguards to prevent capacity from being removed when it will take any subsystem below its minimum required capacity level.” That should prevent someone like Joe from making a similar mistake in the future. \nIn addition, AWS is looking at ways to break down those S3 sub-systems, which were core to the problem into much smaller pieces or cells, as they call them, something they have tried to do in the past. Obviously, the sub-systems proved too large to recover quickly (or at least quickly enough). \nThey close with an apology and a promise to do better. In the end, it was a combination of factors that caused the issue, starting with a human error and then cascading across systems that hadn’t been designed to deal with an error of this magnitude. Featured Image: Colin Anderson /Getty Images 0", "external_links": [], "published": "2017-03-03T01:53:00.000+02:00", "crawled": "2017-03-02T20:59:10.038+02:00", "highlightTitle": ""}