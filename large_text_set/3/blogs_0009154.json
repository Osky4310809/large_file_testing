{"organizations": [], "uuid": "42b9a0975db7188450ddad8f057ea684f97c9e85", "thread": {"social": {"gplus": {"shares": 10}, "pinterest": {"shares": 2}, "vk": {"shares": 0}, "linkedin": {"shares": 41}, "facebook": {"likes": 510, "shares": 510, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "mashable.com", "main_image": "http://i.amz.mshcdn.com/HkIAmO5p1N4aCgdYTEv2tpz4LRs=/1200x630/2017%2F03%2F10%2Fed%2F527c6091a2424359bb0d48a27853294f.26cbf.jpg", "site_section": "http://mashable.com/feed/", "section_title": "Mashable", "url": "http://mashable.com/2017/03/09/artificial-intelligence-suicide-risk/", "country": "US", "domain_rank": 360, "title": "AI figured out the word people text when their suicide risk is high", "performance_score": 5, "site": "mashable.com", "participants_count": 1, "title_full": "AI figured out the word people text when their suicide risk is high", "spam_score": 0.0, "site_type": "blogs", "published": "2017-03-10T07:14:00.000+02:00", "replies_count": 0, "uuid": "42b9a0975db7188450ddad8f057ea684f97c9e85"}, "author": "Rebecca Ruiz", "url": "http://mashable.com/2017/03/09/artificial-intelligence-suicide-risk/", "ord_in_thread": 0, "title": "AI figured out the word people text when their suicide risk is high", "locations": [], "entities": {"persons": [{"name": "charlie brown", "sentiment": "none"}, {"name": "nancy lublin", "sentiment": "none"}, {"name": "julie cerel", "sentiment": "none"}, {"name": "lublin", "sentiment": "none"}, {"name": "cerel", "sentiment": "none"}], "locations": [], "organizations": [{"name": "national suicide prevention lifeline", "sentiment": "none"}, {"name": "ourdatahelps", "sentiment": "none"}, {"name": "facebook", "sentiment": "none"}, {"name": "american association of suicidology", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "If you were asked to guess the words people use when they're most at risk for suicide, you'd be right to think of obvious nouns and verbs like die, overdose and, yes, the word suicide itself. \nSo when Crisis Text Line , a free mental health support service, built an algorithm to flag high-priority texts, it included those among 50 words to indicate the person messaging desperately needed help.\nBut when Crisis Text Line started using artificial intelligence to analyze the 22 million messages about emotional distress in its database last summer, its researchers made a surprising discovery: The word ibuprofen was 16 times more likely to predict the person texting would need emergency services than the word suicide. \nSEE ALSO: Why scientists think your social media posts can help prevent suicide \nAnother highly predictive type of content wasn't even a word but a crying face emoji. When people included that sad character in their messages, Crisis Text Line supervisors were 11 times more likely to call 911 for assistance. In total, Crisis Text Line has integrated 9,000 new words or word combinations that indicate high risk — and expects to add more in the future. \nAnyone know why Charlie Brown is paying an eight-year-old for bad \"advice\" when free empathetic support is available via text at 741741? pic.twitter.com/QV3BG8JhZH \n— Crisis Text Line (@CrisisTextLine) March 7, 2017 \nNancy Lublin, the nonprofit's CEO, says these unexpected data points have made a huge difference. Before AI detected the new words, volunteers responded to high-risk texters in less than two minutes. Now the average response time is down to 39 seconds. Lublin believes that's because the algorithm is much better at identifying those most at risk and sending them to the front of the line, like you would in a hospital emergency room. \n\"We are finally listening to people who are suicidal and using what they’re telling us to figure out how to help them.\" \nJulie Cerel, a clinical psychologist and president-elect of the American Association of Suicidology, says the practical implications of the technology are important. But she also believes the approach reflects a significant change in the way researchers and public health professionals try to prevent suicide. \n\"What this speaks to is we are finally listening to people who are suicidal and using what they’re telling us to figure out how to help them,\" she says. \nIn the past, it's been literally impossible to comb through and code transcripts with suicide attempt survivors at the same scale as Crisis Text Line. Now machine learning makes it possible for researchers to analyze digital conversations and look for signals that someone may be close to attempting suicide. \nThat approach has gained considerable momentum in the last year. Facebook recently announced that it's incorporating AI into its suicide-prevention efforts, and the research project OurDataHelps launched last spring by asking people to \"donate\" their social data so scientists could better understand suicide risk. \nAt Crisis Text Line, conversations that end in what's known as an active rescue are rare. Only 1 percent of those exchanges require intervention by the authorities, and Lublin considers it the last line of defense. The goal, she says, is to help texters create a safety plan and encourage them to feel capable in handling a crisis. But sometimes that approach doesn't work, which is why Lublin wants the system to be as fast and as accurate as possible in singling out high-risk people. \nSupport is available 24/7 directly from our Facebook page. Click “Send Message” at https://t.co/3ryMwEjayz for a live Crisis Counselor.\n— Crisis Text Line (@CrisisTextLine) March 1, 2017 \nEven though the new words and phrases Crisis Text Line identified might not seem immediately useful to doctors and therapists who work with patients in person, Cerel says they're evidence that people don't always choose the most obvious words to talk about suicidal feelings. \n\"It's a reminder to keep asking the question,\" says Cerel, \"and make it clear we want to hear the answer.\" \nIf you want to talk to someone or are experiencing suicidal thoughts, text the Crisis Text Line at 741-741 or call the National Suicide Prevention Lifeline at 1-800-273-8255. Here is a list of international resources. \nWATCH: 'Pokémon Go' is helping some players cope with depression and anxiety", "external_links": [], "published": "2017-03-10T07:14:00.000+02:00", "crawled": "2017-03-10T02:39:04.447+02:00", "highlightTitle": ""}