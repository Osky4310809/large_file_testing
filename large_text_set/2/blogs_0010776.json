{"organizations": [], "uuid": "8c503969e41d9c93f452ef69f2064f1d997d597e", "thread": {"social": {"gplus": {"shares": 63}, "pinterest": {"shares": 2}, "vk": {"shares": 0}, "linkedin": {"shares": 33}, "facebook": {"likes": 963, "shares": 963, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "stats.stackexchange.com", "main_image": "https://cdn.sstatic.net/Sites/stats/img/apple-touch-icon@2.png?v=344f57aa10cc", "site_section": "http://stats.stackexchange.com//feeds", "section_title": "Recent Questions - Cross Validated", "url": "http://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means", "country": "US", "domain_rank": 179, "title": "How to understand the drawbacks of K-means", "performance_score": 9, "site": "stackexchange.com", "participants_count": 1, "title_full": "How to understand the drawbacks of K-means", "spam_score": 0.0, "site_type": "blogs", "published": "2017-02-13T10:34:00.000+02:00", "replies_count": 0, "uuid": "8c503969e41d9c93f452ef69f2064f1d997d597e"}, "author": "KevinKim", "url": "http://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means", "ord_in_thread": 0, "title": "How to understand the drawbacks of K-means", "locations": [], "entities": {"persons": [{"name": "wolpert", "sentiment": "none"}, {"name": "macready", "sentiment": "none"}, {"name": "anscombe", "sentiment": "none"}, {"name": "francis anscombe", "sentiment": "none"}], "locations": [], "organizations": [{"name": "netflix", "sentiment": "none"}, {"name": "sse", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "10 (Just to note - it's probably not a good idea to talk about the \"above answer\", since the answer order that a reader sees can be variable. For instance, if they set the display order to \"active\", then your answer is actually the one above!) – Silverfish Jan 17 '15 at 23:18 @Anony-Mousse This answer is really awesome. But until now, I kind of forget what do we usually mean by saying \"k-means will work under some conditions and will fail under other conditions.\" What do the word \"work\" or \"fail\" mean in this context? Does \"work\" means the solution generated by k-means will visually 'looks reasonable'? This is kind of vague. Or 'work' mean if k-means provide solution which is the same as the 'standard solution' i.e., we pre-generate a data set and use k-means. In this context 'work' makes sense, but in reality, data are not pre-generated by some distribution. – KevinKim Jan 23 '15 at 2:24 Usually people refer to some ground truth, i.e. how the data was generated or to some label hidden from the algorithm. Comparing to generated data will prefer algorithms that optimize the model that was used for generation (e.g. GMM and k-means for Gaussians). And even on real and labeled data this evaluation is about reproducing a known result. When you consider the explorative / knowledge discovery aspect, where you want to learn something new . But it's all we've got. – Anony-Mousse Jan 23 '15 at 7:07 Would it work better on the A3 data set if $k$ were adjusted to the number of effectively present clusters as determined a priori? – TMOTTM Sep 3 '16 at 11:39 @TMOTTM this is with k chosen by prior knowledge. Best of 10 runs all with the \"correct\" k chosen a priori. – Anony-Mousse Sep 3 '16 at 11:43 up vote 339 down vote +100 \nWhat a great question- it's a chance to show how one would inspect the drawbacks and assumptions of any statistical method. Namely: make up some data and try the algorithm on it! \nWe'll consider two of your assumptions, and we'll see what happens to the k-means algorithm when those assumptions are broken. We'll stick to 2-dimensional data since it's easy to visualize. (Thanks to the curse of dimensionality , adding additional dimensions is likely to make these problems more severe, not less). We'll work with the statistical programming language R: you can find the full code here (and the post in blog form here ). Diversion: Anscombe's Quartet \nFirst, an analogy. Imagine someone argued the following: \nI read some material about the drawbacks of linear regression- that it expects a linear trend, that the residuals are normally distributed, and that there are no outliers. But all linear regression is doing is minimizing the sum of squared errors (SSE) from the predicted line. That's an optimization problem that can be solved no matter what the shape of the curve or the distribution of the residuals is. Thus, linear regression requires no assumptions to work. \nWell, yes, linear regression works by minimizing the sum of squared residuals. But that by itself is not the goal of a regression: what we're trying to do is draw a line that serves as a reliable, unbiased predictor of y based on x . The Gauss-Markov theorem tells us that minimizing the SSE accomplishes that goal- but that theorem rests on some very specific assumptions. If those assumptions are broken, you can still minimize the SSE, but it might not do anything. Imagine saying \"You drive a car by pushing the pedal: driving is essentially a 'pedal-pushing process.' The pedal can be pushed no matter how much gas in the tank. Therefore, even if the tank is empty, you can still push the pedal and drive the car.\" \nBut talk is cheap. Let's look at the cold, hard, data. Or actually, made-up data. \nThis is in fact my favorite made-up data: Anscombe's Quartet . Created in 1973 by statistician Francis Anscombe, this delightful concoction illustrates the folly of trusting statistical methods blindly. Each of the datasets has the same linear regression slope, intercept, p-value and $R^2$- and yet at a glance we can see that only one of them, I , is appropriate for linear regression. In II it suggests the wrong shape, in III it is skewed by a single outlier- and in IV there is clearly no trend at all! \nOne could say \"Linear regression is still working in those cases, because it's minimizing the sum of squares of the residuals.\" But what a Pyrrhic victory ! Linear regression will always draw a line, but if it's a meaningless line, who cares? \nSo now we see that just because an optimization can be performed doesn't mean we're accomplishing our goal. And we see that making up data, and visualizing it, is a good way to inspect the assumptions of a model. Hang on to that intuition, we're going to need it in a minute. Broken Assumption: Non-Spherical Data \nYou argue that the k-means algorithm will work fine on non-spherical clusters. Non-spherical clusters like... these? \nMaybe this isn't what you were expecting- but it's a perfectly reasonable way to construct clusters. Looking at this image, we humans immediately recognize two natural groups of points- there's no mistaking them. So let's see how k-means does: assignments are shown in color, imputed centers are shown as X's. \nWell, that 's not right. K-means was trying to fit a square peg in a round hole - trying to find nice centers with neat spheres around them- and it failed. Yes, it's still minimizing the within-cluster sum of squares- but just like in Anscombe's Quartet above, it's a Pyrrhic victory! \nYou might say \"That's not a fair example... no clustering method could correctly find clusters that are that weird.\" Not true! Try single linkage hierachical clustering : \nNailed it! This is because single-linkage hierarchical clustering makes the right assumptions for this dataset. (There's a whole other class of situations where it fails). \nYou might say \"That's a single, extreme, pathological case.\" But it's not! For instance, you can make the outer group a semi-circle instead of a circle, and you'll see k-means still does terribly (and hierarchical clustering still does well). I could come up with other problematic situations easily, and that's just in two dimensions. When you're clustering 16-dimensional data, there's all kinds of pathologies that could arise. \nLastly, I should note that k-means is still salvagable! If you start by transforming your data into polar coordinates , the clustering now works: \nThat's why understanding the assumptions underlying a method is essential: it doesn't just tell you when a method has drawbacks, it tells you how to fix them. Broken Assumption: Unevenly Sized Clusters \nWhat if the clusters have an uneven number of points- does that also break k-means clustering? Well, consider this set of clusters, of sizes 20, 100, 500. I've generated each from a multivariate Gaussian: \nThis looks like k-means could probably find those clusters, right? Everything seems to be generated into neat and tidy groups. So let's try k-means: \nOuch. What happened here is a bit subtler. In its quest to minimize the within-cluster sum of squares, the k-means algorithm gives more \"weight\" to larger clusters. In practice, that means it's happy to let that small cluster end up far away from any center, while it uses those centers to \"split up\" a much larger cluster. \nIf you play with these examples a little ( R code here! ), you'll see that you can construct far more scenarios where k-means gets it embarrassingly wrong. Conclusion: No Free Lunch \nThere's a charming construction in mathematical folklore, formalized by Wolpert and Macready , called the \"No Free Lunch Theorem.\" It's probably my favorite theorem in machine learning philosophy, and I relish any chance to bring it up (did I mention I love this question?) The basic idea is stated (non-rigorously) as this: \"When averaged across all possible situations, every algorithm performs equally well.\" \nSound counterintuitive? Consider that for every case where an algorithm works, I could construct a situation where it fails terribly. Linear regression assumes your data falls along a line- but what if it follows a sinusoidal wave? A t-test assumes each sample comes from a normal distribution: what if you throw in an outlier? Any gradient ascent algorithm can get trapped in local maxima, and any supervised classification can be tricked into overfitting. \nWhat does this mean? It means that assumptions are where your power comes from! When Netflix recommends movies to you, it's assuming that if you like one movie, you'll like similar ones (and vice versa). Imagine a world where that wasn't true, and your tastes are perfectly random- scattered haphazardly across genres, actors and directors. Their recommendation algorithm would fail terribly. Would it make sense to say \"Well, it's still minimizing some expected squared error, so the algorithm is still working\"? You can't make a recommendation algorithm without making some assumptions about users' tastes- just like you can't make a clustering algorithm without making some assumptions about the nature of those clusters. \nSo don't just accept these drawbacks. Know them, so they can inform your choice of algorithms. Understand them, so you can tweak your algorithm and transform your data to solve them. And love them, because if your model could never be wrong, that means it will never be right.", "external_links": [], "published": "2017-02-13T10:34:00.000+02:00", "crawled": "2017-02-13T06:06:42.207+02:00", "highlightTitle": ""}